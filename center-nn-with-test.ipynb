{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "from skimage import io, feature, measure, transform\n",
    "import tensorflow as tf\n",
    "from pandas import Series,DataFrame\n",
    "import sampling as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce(image, scale=4):\n",
    "    reducedimage = transform.pyramid_reduce(image, downscale=scale)\n",
    "    return reducedimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = pd.read_msgpack('training.bin')\n",
    "non_empty_training = training[~training.maskC.isnull()]\n",
    "non_empty_training.index=np.arange(len(non_empty_training))\n",
    "centers = non_empty_training.maskC.as_matrix()\n",
    "center_list=np.array([tuple(c) for c in centers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_centers=center_list[0:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### crop image to 80 x 136 to make the numbers rounder for the nn..\n",
    "def build_training_data(num_images):\n",
    "    data=[]\n",
    "    for i in range(num_images):\n",
    "        if i%100 ==0:\n",
    "            print ('Reading patch ' +str(i))\n",
    "        ultra_image = smp.image_pair(non_empty_training.ix[i].subject,non_empty_training.ix[i].img).image\n",
    "        data.append(reduce(ultra_image)[25:,:136].flatten())\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading patch 0\n",
      "Reading patch 100\n",
      "Reading patch 200\n",
      "Reading patch 300\n",
      "Reading patch 400\n",
      "Reading patch 500\n",
      "Reading patch 600\n",
      "Reading patch 700\n",
      "Reading patch 800\n",
      "Reading patch 900\n",
      "Reading patch 1000\n",
      "Reading patch 1100\n",
      "Reading patch 1200\n",
      "Reading patch 1300\n",
      "Reading patch 1400\n",
      "Reading patch 1500\n",
      "CPU times: user 1min 46s, sys: 8.63 s, total: 1min 55s\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "time X=build_training_data(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "### use 4x reduced images as input features\n",
    "x= tf.placeholder(tf.float32,shape=[None, 10880])\n",
    "x_image = tf.reshape(x, [-1,80,136,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10880,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### helper functions to build Conv and MaxPool layers\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool(x,psize):\n",
    "  return tf.nn.max_pool(x, ksize=[1, psize, psize, 1],\n",
    "                        strides=[1, psize, psize, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### weights and biases for the first Conv layer; 5x5 filter and 32 feat.\n",
    "W_conv1 = weight_variable([7, 7, 1, 32])\n",
    "b_conv1 = bias_variable([32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### build first Conv and MaxPool layers\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool(h_conv1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(27), Dimension(46), Dimension(32)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### stack a second Conv + MaxPool layer; again with 32 feat.\n",
    "W_conv2 = weight_variable([5, 5, 32, 32])\n",
    "b_conv2 = bias_variable([32])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool(h_conv2,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(9), Dimension(16), Dimension(32)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool2.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### stack a third Conv + MaxPool layer; this time 64 feat.\n",
    "W_conv3 = weight_variable([5, 5, 32, 64])\n",
    "b_conv3 = bias_variable([64])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool(h_conv3,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(5), Dimension(8), Dimension(64)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pool3.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### add fully-connected layer\n",
    "\n",
    "W_fc1 = weight_variable([5 * 8 * 64, 256])\n",
    "b_fc1 = bias_variable([256])\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 5*8*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### dropout\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### output layer!\n",
    "W_fc2 = weight_variable([256, 2])\n",
    "b_fc2 = bias_variable([2])\n",
    "\n",
    "y_conv=(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### use least squares cost function\n",
    "cost_fn = tf.reduce_mean(tf.square(y_conv - y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy =  tf.reduce_mean(tf.square(y_conv - y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMS error on training batch0:72167.4\n",
      "RMS error on training batch10:63512.8\n",
      "RMS error on training batch20:56288.7\n",
      "RMS error on training batch30:34714.7\n",
      "RMS error on training batch40:20076.5\n",
      "RMS error on training batch50:3036.51\n",
      "RMS error on training batch60:3580.66\n",
      "RMS error on training batch70:2533.92\n",
      "RMS error on training batch80:2053.8\n",
      "RMS error on training batch90:2157.25\n",
      "RMS error on training batch100:2535.3\n",
      "RMS error on training batch110:2544.65\n",
      "RMS error on training batch120:2386.98\n",
      "RMS error on training batch130:1948.33\n",
      "RMS error on training batch140:2315.38\n",
      "RMS error on training batch150:2656.56\n",
      "RMS error on training batch160:2127.85\n",
      "RMS error on training batch170:2350.73\n",
      "RMS error on training batch180:2098.02\n",
      "RMS error on training batch190:2099.58\n",
      "RMS error on training batch200:2736.4\n",
      "RMS error on training batch210:2277.56\n",
      "RMS error on training batch220:1758.31\n",
      "RMS error on training batch230:2373.1\n",
      "RMS error on training batch240:1798.25\n",
      "RMS error on training batch250:2087.76\n",
      "RMS error on training batch260:1604.84\n",
      "RMS error on training batch270:1917.48\n",
      "RMS error on training batch280:2485.37\n",
      "RMS error on training batch290:2433.51\n",
      "RMS error on training batch300:2473.41\n",
      "RMS error on training batch310:2092.49\n",
      "RMS error on training batch320:1688.93\n",
      "RMS error on training batch330:1555.1\n",
      "RMS error on training batch340:2210.56\n",
      "RMS error on training batch350:1838.88\n",
      "RMS error on training batch360:1906.79\n",
      "RMS error on training batch370:2028.0\n",
      "RMS error on training batch380:2449.07\n",
      "RMS error on training batch390:1547.86\n",
      "RMS error on training batch400:1952.85\n",
      "RMS error on training batch410:1792.78\n",
      "RMS error on training batch420:1945.92\n",
      "RMS error on training batch430:2057.3\n",
      "RMS error on training batch440:2367.83\n",
      "RMS error on training batch450:1882.28\n",
      "RMS error on training batch460:1672.55\n",
      "RMS error on training batch470:2067.66\n",
      "RMS error on training batch480:1637.29\n",
      "RMS error on training batch490:1705.66\n",
      "RMS error on training batch500:1854.82\n",
      "RMS error on training batch510:1540.69\n",
      "RMS error on training batch520:1452.73\n",
      "RMS error on training batch530:1895.99\n",
      "RMS error on training batch540:1683.58\n",
      "RMS error on training batch550:1672.1\n",
      "RMS error on training batch560:1516.37\n",
      "RMS error on training batch570:1692.83\n",
      "RMS error on training batch580:1571.29\n",
      "RMS error on training batch590:1160.31\n",
      "RMS error on training batch600:1779.95\n",
      "RMS error on training batch610:1443.41\n",
      "RMS error on training batch620:1259.44\n",
      "RMS error on training batch630:1678.53\n",
      "RMS error on training batch640:1725.26\n",
      "RMS error on training batch650:1192.77\n",
      "RMS error on training batch660:1286.77\n",
      "RMS error on training batch670:1391.68\n",
      "RMS error on training batch680:1675.81\n",
      "RMS error on training batch690:955.739\n",
      "RMS error on training batch700:1130.28\n",
      "RMS error on training batch710:1465.77\n",
      "RMS error on training batch720:1396.34\n",
      "RMS error on training batch730:2086.98\n",
      "RMS error on training batch740:1590.74\n",
      "RMS error on training batch750:1527.77\n",
      "RMS error on training batch760:1490.42\n",
      "RMS error on training batch770:1610.48\n",
      "RMS error on training batch780:1198.89\n",
      "RMS error on training batch790:1001.47\n",
      "RMS error on training batch800:1704.78\n",
      "RMS error on training batch810:1831.87\n",
      "RMS error on training batch820:1043.92\n",
      "RMS error on training batch830:990.921\n",
      "RMS error on training batch840:1532.25\n",
      "RMS error on training batch850:1350.68\n",
      "RMS error on training batch860:988.501\n",
      "RMS error on training batch870:1743.18\n",
      "RMS error on training batch880:892.704\n",
      "RMS error on training batch890:805.885\n",
      "RMS error on training batch900:864.943\n",
      "RMS error on training batch910:1071.78\n",
      "RMS error on training batch920:1107.04\n",
      "RMS error on training batch930:1382.79\n",
      "RMS error on training batch940:1265.63\n",
      "RMS error on training batch950:751.799\n",
      "RMS error on training batch960:938.857\n",
      "RMS error on training batch970:1109.73\n",
      "RMS error on training batch980:834.729\n",
      "RMS error on training batch990:1094.28\n",
      "RMS error on training batch1000:873.428\n",
      "RMS error on training batch1010:895.049\n",
      "RMS error on training batch1020:639.907\n",
      "RMS error on training batch1030:1402.98\n",
      "RMS error on training batch1040:770.467\n",
      "RMS error on training batch1050:1185.39\n",
      "RMS error on training batch1060:1179.53\n",
      "RMS error on training batch1070:773.492\n",
      "RMS error on training batch1080:937.53\n",
      "RMS error on training batch1090:1067.9\n",
      "RMS error on training batch1100:593.691\n",
      "RMS error on training batch1110:1263.92\n",
      "RMS error on training batch1120:1362.58\n",
      "RMS error on training batch1130:1807.87\n",
      "RMS error on training batch1140:712.348\n",
      "RMS error on training batch1150:782.289\n",
      "RMS error on training batch1160:1093.36\n",
      "RMS error on training batch1170:841.242\n",
      "RMS error on training batch1180:882.351\n",
      "RMS error on training batch1190:1264.96\n",
      "RMS error on training batch1200:723.224\n",
      "RMS error on training batch1210:728.77\n",
      "RMS error on training batch1220:923.746\n",
      "RMS error on training batch1230:738.289\n",
      "RMS error on training batch1240:817.245\n",
      "RMS error on training batch1250:1222.37\n",
      "RMS error on training batch1260:877.314\n",
      "RMS error on training batch1270:928.317\n",
      "RMS error on training batch1280:770.272\n",
      "RMS error on training batch1290:557.576\n",
      "RMS error on training batch1300:666.843\n",
      "RMS error on training batch1310:1007.8\n",
      "RMS error on training batch1320:1255.63\n",
      "RMS error on training batch1330:728.648\n",
      "RMS error on training batch1340:1425.11\n",
      "RMS error on training batch1350:899.918\n",
      "RMS error on training batch1360:888.297\n",
      "RMS error on training batch1370:793.404\n",
      "RMS error on training batch1380:574.881\n",
      "RMS error on training batch1390:797.576\n",
      "RMS error on training batch1400:1390.71\n",
      "RMS error on training batch1410:909.426\n",
      "RMS error on training batch1420:1160.64\n",
      "RMS error on training batch1430:940.198\n",
      "RMS error on training batch1440:1268.92\n",
      "RMS error on training batch1450:988.926\n",
      "RMS error on training batch1460:478.017\n",
      "RMS error on training batch1470:1343.5\n",
      "RMS error on training batch1480:702.645\n",
      "RMS error on training batch1490:912.772\n",
      "RMS error on training batch1500:597.999\n",
      "RMS error on training batch1510:1208.1\n",
      "RMS error on training batch1520:1032.26\n",
      "RMS error on training batch1530:673.947\n",
      "RMS error on training batch1540:942.113\n",
      "RMS error on training batch1550:554.353\n",
      "RMS error on training batch1560:1122.15\n",
      "RMS error on training batch1570:733.789\n",
      "RMS error on training batch1580:638.155\n",
      "RMS error on training batch1590:1018.87\n",
      "RMS error on training batch1600:1080.39\n",
      "RMS error on training batch1610:773.693\n",
      "RMS error on training batch1620:763.543\n",
      "RMS error on training batch1630:630.567\n",
      "RMS error on training batch1640:679.292\n",
      "RMS error on training batch1650:535.18\n",
      "RMS error on training batch1660:993.296\n",
      "RMS error on training batch1670:569.002\n",
      "RMS error on training batch1680:890.506\n",
      "RMS error on training batch1690:936.029\n",
      "RMS error on training batch1700:831.64\n",
      "RMS error on training batch1710:871.914\n",
      "RMS error on training batch1720:1137.94\n",
      "RMS error on training batch1730:772.609\n",
      "RMS error on training batch1740:650.418\n",
      "RMS error on training batch1750:699.54\n",
      "RMS error on training batch1760:981.554\n",
      "RMS error on training batch1770:915.275\n",
      "RMS error on training batch1780:895.966\n",
      "RMS error on training batch1790:948.01\n",
      "RMS error on training batch1800:1157.71\n",
      "RMS error on training batch1810:1485.22\n",
      "RMS error on training batch1820:924.427\n",
      "RMS error on training batch1830:675.538\n",
      "RMS error on training batch1840:845.29\n",
      "RMS error on training batch1850:952.866\n",
      "RMS error on training batch1860:798.768\n",
      "RMS error on training batch1870:748.328\n",
      "RMS error on training batch1880:1080.56\n",
      "RMS error on training batch1890:637.05\n",
      "RMS error on training batch1900:854.504\n",
      "RMS error on training batch1910:912.111\n",
      "RMS error on training batch1920:928.471\n",
      "RMS error on training batch1930:640.756\n",
      "RMS error on training batch1940:723.828\n",
      "RMS error on training batch1950:693.136\n",
      "RMS error on training batch1960:696.445\n",
      "RMS error on training batch1970:1207.66\n",
      "RMS error on training batch1980:1066.42\n",
      "RMS error on training batch1990:887.222\n",
      "RMS error on training batch2000:650.304\n",
      "RMS error on training batch2010:836.444\n",
      "RMS error on training batch2020:518.67\n",
      "RMS error on training batch2030:550.149\n",
      "RMS error on training batch2040:464.345\n",
      "RMS error on training batch2050:635.754\n",
      "RMS error on training batch2060:752.481\n",
      "RMS error on training batch2070:448.506\n",
      "RMS error on training batch2080:907.435\n",
      "RMS error on training batch2090:795.819\n",
      "RMS error on training batch2100:674.41\n",
      "RMS error on training batch2110:407.579\n",
      "RMS error on training batch2120:768.824\n",
      "RMS error on training batch2130:574.038\n",
      "RMS error on training batch2140:1336.04\n",
      "RMS error on training batch2150:606.056\n",
      "RMS error on training batch2160:867.731\n",
      "RMS error on training batch2170:742.446\n",
      "RMS error on training batch2180:833.888\n",
      "RMS error on training batch2190:567.701\n",
      "RMS error on training batch2200:388.398\n",
      "RMS error on training batch2210:779.875\n",
      "RMS error on training batch2220:1103.96\n",
      "RMS error on training batch2230:567.87\n",
      "RMS error on training batch2240:613.717\n",
      "RMS error on training batch2250:529.232\n",
      "RMS error on training batch2260:929.96\n",
      "RMS error on training batch2270:665.41\n",
      "RMS error on training batch2280:780.133\n",
      "RMS error on training batch2290:1029.16\n",
      "RMS error on training batch2300:605.694\n",
      "RMS error on training batch2310:866.447\n",
      "RMS error on training batch2320:546.05\n",
      "RMS error on training batch2330:907.115\n",
      "RMS error on training batch2340:724.144\n",
      "RMS error on training batch2350:824.408\n",
      "RMS error on training batch2360:460.615\n",
      "RMS error on training batch2370:728.58\n",
      "RMS error on training batch2380:684.766\n",
      "RMS error on training batch2390:715.953\n",
      "RMS error on training batch2400:819.744\n",
      "RMS error on training batch2410:703.81\n",
      "RMS error on training batch2420:511.749\n",
      "RMS error on training batch2430:521.255\n",
      "RMS error on training batch2440:702.078\n",
      "RMS error on training batch2450:486.41\n",
      "RMS error on training batch2460:632.205\n",
      "RMS error on training batch2470:598.575\n",
      "RMS error on training batch2480:625.814\n",
      "RMS error on training batch2490:1022.29\n",
      "RMS error on training batch2500:488.619\n",
      "RMS error on training batch2510:593.514\n",
      "RMS error on training batch2520:769.512\n",
      "RMS error on training batch2530:527.972\n",
      "RMS error on training batch2540:1011.63\n",
      "RMS error on training batch2550:602.821\n",
      "RMS error on training batch2560:549.614\n",
      "RMS error on training batch2570:410.555\n",
      "RMS error on training batch2580:500.94\n",
      "RMS error on training batch2590:864.139\n",
      "RMS error on training batch2600:534.044\n",
      "RMS error on training batch2610:415.213\n",
      "RMS error on training batch2620:883.922\n",
      "RMS error on training batch2630:612.057\n",
      "RMS error on training batch2640:355.594\n",
      "RMS error on training batch2650:1490.77\n",
      "RMS error on training batch2660:488.182\n",
      "RMS error on training batch2670:539.541\n",
      "RMS error on training batch2680:360.594\n",
      "RMS error on training batch2690:391.776\n",
      "RMS error on training batch2700:627.371\n",
      "RMS error on training batch2710:749.867\n",
      "RMS error on training batch2720:852.926\n",
      "RMS error on training batch2730:519.659\n",
      "RMS error on training batch2740:442.175\n",
      "RMS error on training batch2750:553.387\n",
      "RMS error on training batch2760:269.245\n",
      "RMS error on training batch2770:636.045\n",
      "RMS error on training batch2780:433.168\n",
      "RMS error on training batch2790:538.199\n",
      "RMS error on training batch2800:433.29\n",
      "RMS error on training batch2810:1052.58\n",
      "RMS error on training batch2820:566.4\n",
      "RMS error on training batch2830:870.332\n",
      "RMS error on training batch2840:764.222\n",
      "RMS error on training batch2850:608.336\n",
      "RMS error on training batch2860:479.927\n",
      "RMS error on training batch2870:642.737\n",
      "RMS error on training batch2880:1003.94\n",
      "RMS error on training batch2890:508.08\n",
      "RMS error on training batch2900:744.449\n",
      "RMS error on training batch2910:482.73\n",
      "RMS error on training batch2920:450.329\n",
      "RMS error on training batch2930:599.151\n",
      "RMS error on training batch2940:610.668\n",
      "RMS error on training batch2950:334.795\n",
      "RMS error on training batch2960:486.146\n",
      "RMS error on training batch2970:569.864\n",
      "RMS error on training batch2980:508.226\n",
      "RMS error on training batch2990:483.626\n",
      "RMS error on training batch3000:576.713\n",
      "RMS error on training batch3010:687.974\n",
      "RMS error on training batch3020:358.193\n",
      "RMS error on training batch3030:769.145\n",
      "RMS error on training batch3040:738.308\n",
      "RMS error on training batch3050:505.45\n",
      "RMS error on training batch3060:489.359\n",
      "RMS error on training batch3070:670.076\n",
      "RMS error on training batch3080:620.29\n",
      "RMS error on training batch3090:671.958\n",
      "RMS error on training batch3100:451.612\n",
      "RMS error on training batch3110:927.255\n",
      "RMS error on training batch3120:453.494\n",
      "RMS error on training batch3130:530.491\n",
      "RMS error on training batch3140:467.248\n",
      "RMS error on training batch3150:562.109\n",
      "RMS error on training batch3160:666.816\n",
      "RMS error on training batch3170:736.564\n",
      "RMS error on training batch3180:855.943\n",
      "RMS error on training batch3190:430.961\n",
      "RMS error on training batch3200:723.715\n",
      "RMS error on training batch3210:820.243\n",
      "RMS error on training batch3220:506.548\n",
      "RMS error on training batch3230:814.74\n",
      "RMS error on training batch3240:504.249\n",
      "RMS error on training batch3250:567.25\n",
      "RMS error on training batch3260:464.217\n",
      "RMS error on training batch3270:867.977\n",
      "RMS error on training batch3280:324.514\n",
      "RMS error on training batch3290:369.741\n",
      "RMS error on training batch3300:488.714\n",
      "RMS error on training batch3310:335.259\n",
      "RMS error on training batch3320:432.738\n",
      "RMS error on training batch3330:483.262\n",
      "RMS error on training batch3340:1179.96\n",
      "RMS error on training batch3350:414.94\n",
      "RMS error on training batch3360:647.776\n",
      "RMS error on training batch3370:529.613\n",
      "RMS error on training batch3380:345.556\n",
      "RMS error on training batch3390:633.62\n",
      "RMS error on training batch3400:503.343\n",
      "RMS error on training batch3410:344.542\n",
      "RMS error on training batch3420:510.74\n",
      "RMS error on training batch3430:712.229\n",
      "RMS error on training batch3440:346.153\n",
      "RMS error on training batch3450:325.608\n",
      "RMS error on training batch3460:785.159\n",
      "RMS error on training batch3470:777.118\n",
      "RMS error on training batch3480:873.496\n",
      "RMS error on training batch3490:430.48\n",
      "RMS error on training batch3500:419.398\n",
      "RMS error on training batch3510:669.716\n",
      "RMS error on training batch3520:592.088\n",
      "RMS error on training batch3530:790.913\n",
      "RMS error on training batch3540:484.686\n",
      "RMS error on training batch3550:406.898\n",
      "RMS error on training batch3560:416.492\n",
      "RMS error on training batch3570:338.916\n",
      "RMS error on training batch3580:559.027\n",
      "RMS error on training batch3590:711.075\n",
      "RMS error on training batch3600:530.63\n",
      "RMS error on training batch3610:476.766\n",
      "RMS error on training batch3620:583.185\n",
      "RMS error on training batch3630:917.35\n",
      "RMS error on training batch3640:531.125\n",
      "RMS error on training batch3650:392.267\n",
      "RMS error on training batch3660:751.652\n",
      "RMS error on training batch3670:442.239\n",
      "RMS error on training batch3680:711.855\n",
      "RMS error on training batch3690:592.147\n",
      "RMS error on training batch3700:721.842\n",
      "RMS error on training batch3710:652.971\n",
      "RMS error on training batch3720:396.076\n",
      "RMS error on training batch3730:477.814\n",
      "RMS error on training batch3740:332.098\n",
      "RMS error on training batch3750:528.937\n",
      "RMS error on training batch3760:415.03\n",
      "RMS error on training batch3770:338.845\n",
      "RMS error on training batch3780:626.716\n",
      "RMS error on training batch3790:518.306\n",
      "RMS error on training batch3800:396.146\n",
      "RMS error on training batch3810:452.571\n",
      "RMS error on training batch3820:334.307\n",
      "RMS error on training batch3830:575.299\n",
      "RMS error on training batch3840:490.409\n",
      "RMS error on training batch3850:495.065\n",
      "RMS error on training batch3860:353.625\n",
      "RMS error on training batch3870:491.956\n",
      "RMS error on training batch3880:647.094\n",
      "RMS error on training batch3890:515.265\n",
      "RMS error on training batch3900:319.776\n",
      "RMS error on training batch3910:381.247\n",
      "RMS error on training batch3920:487.045\n",
      "RMS error on training batch3930:417.532\n",
      "RMS error on training batch3940:376.69\n",
      "RMS error on training batch3950:693.608\n",
      "RMS error on training batch3960:276.883\n",
      "RMS error on training batch3970:265.864\n",
      "RMS error on training batch3980:424.311\n",
      "RMS error on training batch3990:656.245\n",
      "RMS error on training batch4000:523.581\n",
      "RMS error on training batch4010:326.872\n",
      "RMS error on training batch4020:503.601\n",
      "RMS error on training batch4030:499.602\n",
      "RMS error on training batch4040:434.325\n",
      "RMS error on training batch4050:546.48\n",
      "RMS error on training batch4060:679.609\n",
      "RMS error on training batch4070:417.52\n",
      "RMS error on training batch4080:607.113\n",
      "RMS error on training batch4090:453.69\n",
      "RMS error on training batch4100:388.02\n",
      "RMS error on training batch4110:268.3\n",
      "RMS error on training batch4120:805.587\n",
      "RMS error on training batch4130:328.432\n",
      "RMS error on training batch4140:1008.5\n",
      "RMS error on training batch4150:762.352\n",
      "RMS error on training batch4160:394.834\n",
      "RMS error on training batch4170:596.483\n",
      "RMS error on training batch4180:334.294\n",
      "RMS error on training batch4190:359.067\n",
      "RMS error on training batch4200:570.113\n",
      "RMS error on training batch4210:413.936\n",
      "RMS error on training batch4220:601.849\n",
      "RMS error on training batch4230:313.434\n",
      "RMS error on training batch4240:278.654\n",
      "RMS error on training batch4250:375.588\n",
      "RMS error on training batch4260:695.014\n",
      "RMS error on training batch4270:832.348\n",
      "RMS error on training batch4280:467.523\n",
      "RMS error on training batch4290:418.047\n",
      "RMS error on training batch4300:387.966\n",
      "RMS error on training batch4310:810.92\n",
      "RMS error on training batch4320:535.578\n",
      "RMS error on training batch4330:888.473\n",
      "RMS error on training batch4340:467.161\n",
      "RMS error on training batch4350:259.984\n",
      "RMS error on training batch4360:380.706\n",
      "RMS error on training batch4370:560.766\n",
      "RMS error on training batch4380:506.755\n",
      "RMS error on training batch4390:349.549\n",
      "RMS error on training batch4400:280.799\n",
      "RMS error on training batch4410:274.859\n",
      "RMS error on training batch4420:607.463\n",
      "RMS error on training batch4430:531.678\n",
      "RMS error on training batch4440:723.447\n",
      "RMS error on training batch4450:343.857\n",
      "RMS error on training batch4460:333.259\n",
      "RMS error on training batch4470:629.02\n",
      "RMS error on training batch4480:764.82\n",
      "RMS error on training batch4490:365.945\n",
      "RMS error on training batch4500:344.009\n",
      "RMS error on training batch4510:373.394\n",
      "RMS error on training batch4520:417.071\n",
      "RMS error on training batch4530:327.669\n",
      "RMS error on training batch4540:217.291\n",
      "RMS error on training batch4550:445.445\n",
      "RMS error on training batch4560:376.605\n",
      "RMS error on training batch4570:422.827\n",
      "RMS error on training batch4580:391.571\n",
      "RMS error on training batch4590:354.826\n",
      "RMS error on training batch4600:570.965\n",
      "RMS error on training batch4610:395.588\n",
      "RMS error on training batch4620:422.008\n",
      "RMS error on training batch4630:430.66\n",
      "RMS error on training batch4640:260.451\n",
      "RMS error on training batch4650:812.77\n",
      "RMS error on training batch4660:349.248\n",
      "RMS error on training batch4670:481.958\n",
      "RMS error on training batch4680:372.155\n",
      "RMS error on training batch4690:645.289\n",
      "RMS error on training batch4700:557.955\n",
      "RMS error on training batch4710:475.154\n",
      "RMS error on training batch4720:485.872\n",
      "RMS error on training batch4730:460.491\n",
      "RMS error on training batch4740:314.717\n",
      "RMS error on training batch4750:330.061\n",
      "RMS error on training batch4760:229.337\n",
      "RMS error on training batch4770:458.569\n",
      "RMS error on training batch4780:470.36\n",
      "RMS error on training batch4790:440.267\n",
      "RMS error on training batch4800:436.49\n",
      "RMS error on training batch4810:354.712\n",
      "RMS error on training batch4820:624.044\n",
      "RMS error on training batch4830:376.315\n",
      "RMS error on training batch4840:316.984\n",
      "RMS error on training batch4850:462.449\n",
      "RMS error on training batch4860:641.806\n",
      "RMS error on training batch4870:315.965\n",
      "RMS error on training batch4880:519.893\n",
      "RMS error on training batch4890:352.148\n",
      "RMS error on training batch4900:444.048\n",
      "RMS error on training batch4910:247.861\n",
      "RMS error on training batch4920:355.931\n",
      "RMS error on training batch4930:581.714\n",
      "RMS error on training batch4940:429.021\n",
      "RMS error on training batch4950:403.616\n",
      "RMS error on training batch4960:392.678\n",
      "RMS error on training batch4970:253.801\n",
      "RMS error on training batch4980:283.768\n",
      "RMS error on training batch4990:401.252\n",
      "RMS error on training batch5000:325.771\n",
      "RMS error on training batch5010:448.018\n",
      "RMS error on training batch5020:238.786\n",
      "RMS error on training batch5030:648.494\n",
      "RMS error on training batch5040:247.219\n",
      "RMS error on training batch5050:458.075\n",
      "RMS error on training batch5060:473.277\n",
      "RMS error on training batch5070:495.788\n",
      "RMS error on training batch5080:533.749\n",
      "RMS error on training batch5090:330.268\n",
      "RMS error on training batch5100:283.162\n",
      "RMS error on training batch5110:412.469\n",
      "RMS error on training batch5120:371.346\n",
      "RMS error on training batch5130:467.801\n",
      "RMS error on training batch5140:197.918\n",
      "RMS error on training batch5150:345.292\n",
      "RMS error on training batch5160:233.991\n",
      "RMS error on training batch5170:360.894\n",
      "RMS error on training batch5180:397.643\n",
      "RMS error on training batch5190:376.862\n",
      "RMS error on training batch5200:330.316\n",
      "RMS error on training batch5210:265.151\n",
      "RMS error on training batch5220:501.322\n",
      "RMS error on training batch5230:546.669\n",
      "RMS error on training batch5240:252.22\n",
      "RMS error on training batch5250:425.821\n",
      "RMS error on training batch5260:502.253\n",
      "RMS error on training batch5270:625.803\n",
      "RMS error on training batch5280:572.654\n",
      "RMS error on training batch5290:319.036\n",
      "RMS error on training batch5300:314.074\n",
      "RMS error on training batch5310:270.803\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-064d2f4b25d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         x:batch[0], y_: batch[1], keep_prob: 1.0})\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'RMS error on training batch'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/CDIPS/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \"\"\"\n\u001b[0;32m-> 1377\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CDIPS/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3130\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3131\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3132\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CDIPS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CDIPS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 511\u001b[0;31m                            feed_dict_string)\n\u001b[0m\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CDIPS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 564\u001b[0;31m                            target_list)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/envs/CDIPS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    569\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m       \u001b[0me_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/CDIPS/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    pts = np.random.randint(0,1000,size=50)\n",
    "    batch = (X[pts].flatten().reshape(50,10880), training_centers[pts].flatten().reshape(50,2))\n",
    "    if i%10 ==0:\n",
    "        train_error = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print ('RMS error on training batch'+str(i)+ ':'+str(train_error))\n",
    "    train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob: 0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_test_data(num_images):\n",
    "    data=[]\n",
    "    for i in range(2000,2300):\n",
    "        #print(i)\n",
    "        if i%100 ==0:\n",
    "            print ('Reading patch '+str(i))\n",
    "        ultra_image = smp.image_pair(non_empty_training.ix[i].subject,non_empty_training.ix[i].img).image\n",
    "        data.append(reduce(ultra_image)[25:,:136].flatten())\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saved_model  = saver.save(sess, \"/Users/gus/CDIPS/uns/centers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading patch 2000\n",
      "Reading patch 2100\n",
      "Reading patch 2200\n",
      "CPU times: user 21.1 s, sys: 1.93 s, total: 23.1 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "time x_test=build_test_data(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 10880)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_centers=center_list[2000:2300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### looks like we're overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rms error 1351.45\n"
     ]
    }
   ],
   "source": [
    "print(\"test rms error %g\"%accuracy.eval(feed_dict={\n",
    "    x: x_test, y_: test_centers, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [CDIPS]",
   "language": "python",
   "name": "Python [CDIPS]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
